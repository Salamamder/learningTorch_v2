{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c652e158-1f13-4405-9c7c-c6203258ca39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAM optimizer 테스트\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78038c38-dc70-4dc8-9376-39e0069e4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(5000, 10) # 10개의 feature가 있는 5000 개의 데이터\n",
    "y = torch.zeros(5000, 1)\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "minibatch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf8f299c-0957-4c6c-b485-cb3d5900a634",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = x.size(-1)\n",
    "output_dim = y.size(-1)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0,1),\n",
    "    nn.Linear(10,8),\n",
    "    nn.LeakyReLU(0,1),\n",
    "    nn.Linear(8,6),\n",
    "    nn.LeakyReLU(0,1),\n",
    "    nn.Linear(6, output_dim)\n",
    ")\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "oprimizer = torch.optim.Adam(model.parameters()) # Adam은 learning rate 가 필요 없음, SGD는 피룡함\n",
    "# adam모델을 가장 많이 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6da8581-d4c8-4435-afb3-b8753c0a6ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3608, 1231,  238,  ..., 4539, 1179, 1824])\n",
      "20 20\n"
     ]
    }
   ],
   "source": [
    "# 미니배치를 만드는 것\n",
    "indices = torch.randperm(x.size(0))\n",
    "print(indices)\n",
    "x_batch_list = torch.index_select(x, 0, index = indices)\n",
    "y_batch_list = torch.index_select(y, 0, index = indices)\n",
    "x_batch_list = x_batch_list.split(minibatch_size, dim=0) # minibatch로 잘려서 [] 에서 [[],[],[]...] 아러한 형태로 바\n",
    "y_batch_list = y_batch_list.split(minibatch_size, dim=0)\n",
    "print(len(x_batch_list), len(y_batch_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f5e92b-55b2-44d0-ae18-106c73091997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.1876,  0.0953,  0.0216, -0.2204,  0.1129,  0.1977,  0.1804, -0.1209,\n",
      "         -0.3104,  0.2676],\n",
      "        [-0.2784,  0.1095,  0.2615, -0.1662, -0.2879, -0.1037,  0.2124, -0.2885,\n",
      "          0.0685, -0.1541],\n",
      "        [-0.2349, -0.0624, -0.0422, -0.2477,  0.1337,  0.0155, -0.1729, -0.2676,\n",
      "          0.1140, -0.1250],\n",
      "        [-0.2975, -0.2216,  0.3099, -0.2528, -0.2747, -0.3051, -0.0590,  0.1243,\n",
      "          0.0431, -0.0986],\n",
      "        [ 0.1421, -0.0747, -0.1905, -0.0706,  0.2504, -0.2202, -0.1902, -0.2211,\n",
      "          0.2742, -0.0883],\n",
      "        [-0.0356, -0.2966, -0.2881, -0.0213,  0.2192, -0.2798,  0.1919, -0.1688,\n",
      "         -0.0667,  0.0278],\n",
      "        [ 0.2715,  0.2532,  0.2579, -0.2067, -0.2403, -0.3376,  0.1507, -0.1608,\n",
      "         -0.1452,  0.2153],\n",
      "        [-0.1031,  0.3420,  0.1060,  0.3246, -0.1867,  0.3391, -0.2362, -0.2628,\n",
      "         -0.0189,  0.2158],\n",
      "        [ 0.2384, -0.1385,  0.0973,  0.3487,  0.1827, -0.1957,  0.0798,  0.1778,\n",
      "          0.2033,  0.0111],\n",
      "        [ 0.1664,  0.3119, -0.2736,  0.0527, -0.0202, -0.0338, -0.1290, -0.0707,\n",
      "         -0.0672, -0.2148]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0049,  0.1970,  0.1502,  0.3101, -0.0732,  0.2205, -0.1521, -0.0509,\n",
      "        -0.0434, -0.1847], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0690,  0.2626,  0.2329,  0.2311, -0.2971,  0.0266, -0.1281, -0.0818,\n",
      "         -0.2950, -0.0955],\n",
      "        [ 0.2318, -0.1821, -0.0095, -0.1417, -0.2136,  0.2616,  0.1115,  0.0389,\n",
      "          0.1709,  0.2353],\n",
      "        [ 0.2658,  0.0494,  0.0405,  0.3089,  0.1112,  0.2999,  0.0144,  0.2276,\n",
      "          0.0581,  0.1958],\n",
      "        [-0.1839,  0.2378, -0.2276,  0.0191, -0.3080, -0.0604, -0.1292,  0.2585,\n",
      "         -0.2230,  0.1081],\n",
      "        [-0.0373,  0.2186, -0.2703, -0.1940, -0.0210, -0.1464,  0.2048,  0.2483,\n",
      "          0.2409,  0.2607],\n",
      "        [-0.2976,  0.0133,  0.1460, -0.0797,  0.2719,  0.3012, -0.2743, -0.1158,\n",
      "          0.0763, -0.1695],\n",
      "        [ 0.0737,  0.2076,  0.2075,  0.1897,  0.0547,  0.0615, -0.2662,  0.1811,\n",
      "          0.2913,  0.1180],\n",
      "        [-0.0634,  0.2866,  0.2322, -0.2955,  0.3023, -0.1642,  0.0816, -0.1446,\n",
      "          0.0593, -0.1129]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0672,  0.1636,  0.0219, -0.2971,  0.3348, -0.3144,  0.0247,  0.2665],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0552, -0.1653, -0.1042,  0.1675,  0.1742,  0.2694,  0.2569,  0.2261],\n",
      "        [-0.1036, -0.1566, -0.1525, -0.2007, -0.1667, -0.1001,  0.0866,  0.2063],\n",
      "        [-0.3083,  0.0942,  0.1209, -0.0699,  0.1632,  0.1959, -0.0031,  0.1863],\n",
      "        [-0.1724, -0.2323,  0.1920,  0.0285, -0.2654,  0.1418,  0.1893,  0.3075],\n",
      "        [-0.0330,  0.0583,  0.2012, -0.0778,  0.2676,  0.1521,  0.3621, -0.3182],\n",
      "        [-0.2416, -0.0864, -0.0014, -0.3334,  0.3378,  0.1421,  0.2004, -0.0233]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2874, -0.1367,  0.2558,  0.0024,  0.1849, -0.0984],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3680,  0.2517,  0.2526,  0.2255, -0.3140, -0.2674]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.2596], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for index in range(nb_epochs):\n",
    "    indices = torch.randperm(x.size(0))\n",
    "\n",
    "    x_batch_list = torch.index_select(x, 0, index = indices)\n",
    "    y_batch_list = torch.index_select(y, 0, index = indices)\n",
    "    x_batch_list = x_batch_list.split(minibatch_size,0)\n",
    "    y_batch_list = y_batch_list.split(minibatch_size,0)\n",
    "\n",
    "    for x_minibatch, y_minibatch in zip(x_batch_list, y_batch_list):\n",
    "        y_minibatch_pred = model(x_minibatch)\n",
    "        loss = loss_function(y_minibatch_pred, y_minibatch)\n",
    "\n",
    "        oprimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        oprimizer.step()\n",
    "\n",
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "402729bb-7a7b-4269-a926-784ab3a234f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2226,  677,  497,  ...,  518,   84, 3923], device='cuda:0')\n",
      "20 20\n",
      "tensor(0., device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0632, -0.1414,  0.1404, -0.0282,  0.0084,  0.0784,  0.1894,  0.0612,\n",
      "          0.0196, -0.0827],\n",
      "        [ 0.2602, -0.0773,  0.1612, -0.1517,  0.2113,  0.1296, -0.0737, -0.3119,\n",
      "          0.2526, -0.0415],\n",
      "        [ 0.1612, -0.1563,  0.0577, -0.2659, -0.0224,  0.1242, -0.2874, -0.2807,\n",
      "         -0.1709,  0.2236],\n",
      "        [-0.2371, -0.2360,  0.0559,  0.1714, -0.0860, -0.0600, -0.1414,  0.1164,\n",
      "         -0.0808, -0.1345],\n",
      "        [-0.1531, -0.0543,  0.2924,  0.2891,  0.0889,  0.1442, -0.1004,  0.1887,\n",
      "         -0.0801, -0.1334],\n",
      "        [-0.1101,  0.1824, -0.1669,  0.0430,  0.1436, -0.2186, -0.0727, -0.0910,\n",
      "         -0.0143, -0.1440],\n",
      "        [ 0.1189, -0.1547, -0.0036, -0.2904, -0.2642,  0.1655, -0.0483, -0.2525,\n",
      "         -0.0325,  0.2219],\n",
      "        [-0.0918,  0.2279, -0.2246,  0.1607, -0.2447, -0.2739,  0.1632,  0.1900,\n",
      "         -0.1837, -0.2676],\n",
      "        [ 0.2389,  0.1197,  0.0532,  0.2915, -0.0727, -0.2289,  0.2544,  0.1216,\n",
      "          0.1572, -0.0664],\n",
      "        [ 0.2918,  0.2785, -0.0758, -0.0453, -0.0602,  0.0278,  0.3128, -0.2915,\n",
      "         -0.0714,  0.2159]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1059, -0.3047,  0.2866,  0.2394, -0.2530,  0.0214,  0.2880,  0.2562,\n",
      "         0.2568,  0.1982], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3082, -0.0500,  0.0035,  0.0668, -0.3032,  0.0483,  0.0396, -0.0301,\n",
      "         -0.0133, -0.1878],\n",
      "        [-0.1022,  0.1464,  0.2170, -0.0117, -0.1092,  0.0698, -0.2657, -0.3082,\n",
      "         -0.0022, -0.0781],\n",
      "        [ 0.1940,  0.2026, -0.1942, -0.1249,  0.2942,  0.2086, -0.3088, -0.1232,\n",
      "          0.1327, -0.0447],\n",
      "        [ 0.2258, -0.1423,  0.2728, -0.1564, -0.0215,  0.0835,  0.0599, -0.1249,\n",
      "          0.2503, -0.2442],\n",
      "        [-0.0181, -0.0880,  0.1556, -0.1836, -0.2837, -0.0259,  0.0866, -0.0040,\n",
      "          0.1366,  0.2007],\n",
      "        [ 0.3036,  0.2361,  0.1087, -0.2698,  0.2366,  0.0910, -0.2025,  0.1510,\n",
      "         -0.1411, -0.2872],\n",
      "        [-0.2650,  0.1853,  0.0590,  0.2360, -0.1611,  0.0110, -0.2493, -0.1631,\n",
      "         -0.1278,  0.1628],\n",
      "        [ 0.0780,  0.0364,  0.2391,  0.2534, -0.2107,  0.0718, -0.0708,  0.2507,\n",
      "          0.1417, -0.2565]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0785,  0.1828,  0.1873, -0.2148,  0.1833, -0.2644,  0.2461, -0.1317],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2479,  0.2758, -0.2668, -0.0106,  0.0288,  0.0081, -0.1778, -0.3248],\n",
      "        [-0.1743,  0.1192,  0.1375,  0.0969,  0.0575, -0.1444,  0.3304, -0.1573],\n",
      "        [-0.1844, -0.2553,  0.1989, -0.3169,  0.3353, -0.1230,  0.2820,  0.1028],\n",
      "        [ 0.1610,  0.1877,  0.1244,  0.0116,  0.0909,  0.2485,  0.1857, -0.0755],\n",
      "        [ 0.2600, -0.0961,  0.3223, -0.1136,  0.0705,  0.2589, -0.0167,  0.0953],\n",
      "        [-0.0821,  0.0420, -0.2046, -0.1694, -0.3441, -0.2100, -0.0935,  0.0815]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0910,  0.0690,  0.2702, -0.1014, -0.0733, -0.1216], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3998,  0.3190, -0.3846,  0.3064,  0.0885,  0.2682]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1145], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# ADAM optimizer 테스트\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "x = torch.ones(5000, 10).to(device)\n",
    "y = torch.zeros(5000, 1).to(device)\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "minibatch_size = 256\n",
    "\n",
    "input_dim = x.size(-1)\n",
    "output_dim = y.size(-1)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0,1),\n",
    "    nn.Linear(10,8),\n",
    "    nn.LeakyReLU(0,1),\n",
    "    nn.Linear(8,6),\n",
    "    nn.LeakyReLU(0,1),\n",
    "    nn.Linear(6, output_dim)\n",
    ").to(device)\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "oprimizer = torch.optim.Adam(model.parameters()) # Adam은 learning rate 가 필요 없음, SGD는 피룡함\n",
    "# adam모델을 가장 많이 사용\n",
    "\n",
    "# 미니배치를 만드는 것\n",
    "indices = torch.randperm(x.size(0), device=device)\n",
    "print(indices)\n",
    "x_batch_list = torch.index_select(x, 0, index = indices)\n",
    "y_batch_list = torch.index_select(y, 0, index = indices)\n",
    "x_batch_list = x_batch_list.split(minibatch_size, dim=0) # minibatch로 잘려서 [] 에서 [[],[],[]...] 아러한 형태로 바\n",
    "y_batch_list = y_batch_list.split(minibatch_size, dim=0)\n",
    "print(len(x_batch_list), len(y_batch_list))\n",
    "\n",
    "for index in range(nb_epochs):\n",
    "    indices = torch.randperm(x.size(0), device=device)\n",
    "\n",
    "    x_batch_list = torch.index_select(x, 0, index = indices)\n",
    "    y_batch_list = torch.index_select(y, 0, index = indices)\n",
    "    x_batch_list = x_batch_list.split(minibatch_size,0)\n",
    "    y_batch_list = y_batch_list.split(minibatch_size,0)\n",
    "\n",
    "    for x_minibatch, y_minibatch in zip(x_batch_list, y_batch_list):\n",
    "        y_minibatch_pred = model(x_minibatch)\n",
    "        loss = loss_function(y_minibatch_pred, y_minibatch)\n",
    "\n",
    "        oprimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        oprimizer.step()\n",
    "\n",
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccfeee-ec38-4491-b35f-9f555d0b96df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
