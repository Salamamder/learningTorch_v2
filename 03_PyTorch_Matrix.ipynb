{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb82456-e9f5-4759-8f1e-74d06bbf2390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형대수 과목에서 익힘 -> 텐서에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14856081-72f5-4215-8394-a26c70da5d4d",
   "metadata": {},
   "source": [
    "torch.matmul() 메서드는 2D텐서 3D텐서간의 연산만 지원, 일반 matmul()과의 차이점을 알아둘 필요 있음 <br>\n",
    "matmul()은 텐서의 shape등에 따라, 다양한 계산이 가능, broadcasting도 지원하므로, 자칫 예상치못한 연산이 될 수도 있다. <br>\n",
    "디버깅을 위해, 기대한 케이스에 대해서만 명확한 계산을 하는 것이 필요하다면, torch.mm(), torch.bmm() 사용을 고려해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b574703e-703f-469d-aa48-e99e43377838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e956d4-f2d1-42c2-b808-7c90cd284a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.full((3,), 2) # 벡터 생성\n",
    "B = torch.full((3,), 3) # 벡터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f15ff57-43a8-476d-9373-ed0b659f1c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2]) 1 torch.Size([3])\n",
      "tensor([3, 3, 3]) 1 torch.Size([3])\n",
      "tensor(18) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(A, A.dim(), A.shape)\n",
    "print(B, B.dim(), B.shape)\n",
    "result = torch.matmul(A,B) # 1차원 텐서의 곱, 각 요소들 곱하고 더하기\n",
    "print(result, result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a48848f-7309-4402-a66f-fe3563320dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18, 18, 18],\n",
      "        [18, 18, 18]])\n"
     ]
    }
   ],
   "source": [
    "# 2차원 텐서의 곱 = 행렬 곱\n",
    "A = torch.full((2,3), 2)\n",
    "B = torch.full((3,3), 3)\n",
    "result = torch.matmul(A,B)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc6a6110-93c7-4880-bf56-a2070bb90e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 18])\n",
      "torch.Size([2])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# 벡터 X 행열 -> 벡터를 matrix에 맞게 transpose 해준다. (matmul에서 자동으로 해줌)\n",
    "A = torch.full((3,), 2)\n",
    "B = torch.full((3,2), 3)\n",
    "result = torch.matmul(A, B)\n",
    "print(result) # 결과로 벡터가 나옴 1 x 2 형태 행 벡터\n",
    "print(result.size())\n",
    "print(result.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79dd28ca-956f-4287-bd28-0bc0ae517250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18, 18],\n",
      "        [18, 18],\n",
      "        [18, 18],\n",
      "        [18, 18]])\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "# 벡터 x 3차원 이상의 텐서 => 뭘까\n",
    "# (a) x (b,c,d) 형태에서 (b,c,d) = b x (c, d)\n",
    "# = (b) x ((a) x (c, d)) => (b) x (d) => (b, d)\n",
    "A = torch.full((3,), 2)\n",
    "B = torch.full((4,3,2),3)\n",
    "result = torch.matmul(A, B)\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f953f25-8299-4efc-9e8c-2e68bcfb21dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12, 12, 12])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 곱셈의 순서도 중요한데 \n",
    "# 행렬 x 벡터 의 경우 위에서 배운 벡터x행렬과는 거의 비슷\n",
    "A = torch.full((3,2), 3)\n",
    "B = torch.full((2,), 2)\n",
    "result = torch.matmul(A, B)\n",
    "print(result)\n",
    "print(result.shape)# (3, 2) x (2, 1) => 3, 1 => (3) 이렇게 된다. (거의 비슷)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b61fe59-bf65-4cdc-8ad5-880414bcd021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12],\n",
      "        [12, 12, 12]])\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "# 3차원 이상의 텐서\n",
    "A = torch.full((4,3,2), 3)\n",
    "B = torch.full((2,), 2)\n",
    "result = torch.matmul(A, B)\n",
    "print(result)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8614980-ce16-4639-acc6-cf393dc118f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3차원텐서 x 벡터 (설)\n",
    "# (a,b,c) x (d)\n",
    "# a x (b, c) x (d) => 여기서 c와 d가 같아야 계산이 성립함\n",
    "# a x (b, c) x (c,) = a x (b) = (a, b) 이렇게 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9a132-c6b1-4c9c-aa27-f4c799730fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mm 행렬 곱만 지원\n",
    "# torch.bmm 3d 텐서 끼리 곱만 지원\n",
    "# 위의 두 개는 브로드캐스팅같은거 지원 x \n",
    "# 장점: 원하지 않는 곱 안해줌\n",
    "# 단점: static하게만 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc42c2a-0a4c-4e91-9a20-6e42dd39e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dea47b3-98bf-45b8-b99e-8fe48dd6c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(4) # 입력\n",
    "w = torch.FloatTensor(4,3) # 가중치\n",
    "b = torch.FloatTensor(3) # 편향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "462abadc-2ef5-4700-af83-48fdfa4cfaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearfunction(x,w,b):\n",
    "    y = torch.matmul(x,w) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674f4a77-7de5-4b43-9432-3d3e6045a6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.8870e+10, 2.7615e-37, 0.0000e+00])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "y = linearfunction(x,w,b)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7b7f0fe-2c20-40b9-a964-270b71f1394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신경망 모델 클래스를 만들고, nn.Module 을 상속받는다.\n",
    "# __init__ 에서 신경망 계층 초기화를 선언한다.\n",
    "# forward() 메서드에서 입력 데이터에 대한 연산을 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f74200-d174-4547-b1d7-948edbb43bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085f92f4-a768-4cb3-b855-c84efb78e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = torch.FloatTensor(4, 3) # 입력차원과 출력차원을 써줘야함\n",
    "        self.b = torch.FloatTensor(3) # 편향, 출력차원 수 만큼 필요함\n",
    "    def forward(self, x):\n",
    "        y = torch.matmul(x, self.w) + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e80fb65-4455-4983-b2ee-2e6e091fc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(4)\n",
    "mylinear = NeuralNetwork()\n",
    "y = mylinear(x) # .forward 를 안써도 알아서 forward로 호출해서 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1da8bb32-224a-4dab-a7ea-b0fc5f9395a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3af9ab49-377a-4323-9b57-93842d935c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 연습\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "class NeuralNetwork(nn.Module): # 상속받는거임\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.w = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "\n",
    "    def forward(self, x): # 오버라이딩 하는거\n",
    "        y = torch.matmul(x, self.w) + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6157ff0-0a7b-4d6e-a1e1-52f6ae4195a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9673e+05, 1.4041e-42, 0.0000e+00])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# 실행 연습\n",
    "x = torch.FloatTensor(5)\n",
    "mylinear = NeuralNetwork(5,3)\n",
    "y = mylinear(x)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b6eb545-5f9e-419d-9da8-829abf5d8763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위의 것보다 파라미터 정하는 것을 실전에 가깝게 만든거\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module): # 상속받는거임\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.FloatTensor(input_dim, output_dim)) # 변수가 파라미터로 등록되게 하는 것\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "\n",
    "    def forward(self, x): # 오버라이딩 하는거\n",
    "        y = torch.matmul(x, self.w) + self.b\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cd83fde-a5bb-4ff6-967e-8f3c616e406e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 2.2798, 0.0000], grad_fn=<AddBackward0>)\n",
      "torch.Size([3])\n",
      "------------------------------------------------------------\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0000, 2.2798, 0.0000], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(4)\n",
    "mylinear = NeuralNetwork(4,3)\n",
    "y = mylinear(x)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "print(\"------\"*10)\n",
    "for param in mylinear.parameters():\n",
    "    print(param)\n",
    "# requires_grad=True : 파라미터 업데이트 설정 on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7fe4e62-53aa-4c60-bc06-295f734daad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 11788.3379,  90582.8281, -24404.5332], grad_fn=<ViewBackward0>)\n",
      "torch.Size([3])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0587, -0.3503, -0.3891, -0.2417],\n",
      "        [ 0.4513, -0.4304, -0.0329, -0.0826],\n",
      "        [-0.1216, -0.4168,  0.4741, -0.1038]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3479, -0.4481, -0.4123], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# y = torch.matmul(x, self.w) + self.b \n",
    "# 위에 식 자주 써서 그런지 모듈로 만들어져 있음\n",
    "# nn.Linear 클래스\n",
    "\n",
    "mylinear = nn.Linear(4,3)\n",
    "y = mylinear(x)\n",
    "print(y)\n",
    "print(y.shape)\n",
    "\n",
    "for param in mylinear.parameters():\n",
    "    print(param)\n",
    "\n",
    "# 실제 파이토치 모델은 여러가지 최적화, 설정 때문에 클래스로 오버라이딩 하는 경우가 흔하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ce4802-9437-46f7-a6fd-81f30a303bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저(Optimizer) \n",
    "# 손실값을 적게 만들기 위해 사용하는 알고리즘\n",
    "# GD, BGD, SGD 등등 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72eb53bf-edb3-4a4b-a6c9-2aac8796c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGD 문제점: 안정적으로 출력을 뽑으나 global minimum이 아닌 local minimum을 찾는 경향이 있음\n",
    "# SGD : 전체중 랜덤한 데이터를 뽑아서 업데이트를 가져감\n",
    "# mini batch gradient descent와 비슷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1eff2a-9c07-44e2-af7e-c4fc4a7590c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차 역전파 : 편미분, chain rule을 이용한 것\n",
    "# 조건\n",
    "# requires_grad = True & (backward() 호출)\n",
    "# requires_grad = True 이걸로 계산을 추적함 => chain rule을 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4619d1d-f73d-406c-9344-4e1edb117fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.rand(1, requires_grad = True)\n",
    "y = torch.rand(1)\n",
    "y.requires_grad = True\n",
    "loss = y - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef3a677c-7337-406c-94f4-26dbc7e39c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5418], requires_grad=True)\n",
      "tensor([0.8156], requires_grad=True)\n",
      "tensor([0.2738], grad_fn=<SubBackward0>)\n",
      "None\n",
      "tensor([-1.]) tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "print(loss)\n",
    "print(loss.backward())\n",
    "print(x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b77d29b5-41f6-40a0-b1a3-a0afdf41b479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5185, 0.0203, 0.1158],\n",
      "        [0.8933, 0.5116, 0.0357],\n",
      "        [0.6309, 0.1844, 0.7182],\n",
      "        [0.2833, 0.1977, 0.5367]], requires_grad=True)\n",
      "tensor([0.0085, 0.9787, 0.7700], requires_grad=True)\n",
      "tensor([2.3346, 1.8927, 2.1765], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "w = torch.rand(4,3, requires_grad = True)\n",
    "b = torch.rand(3, requires_grad = True)\n",
    "z = torch.matmul(x,w) + b\n",
    "print(w)\n",
    "print(b)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dabc6a4-2cc5-4407-9671-17586189b543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5564, 1.2618, 1.4510],\n",
      "        [1.5564, 1.2618, 1.4510],\n",
      "        [1.5564, 1.2618, 1.4510],\n",
      "        [1.5564, 1.2618, 1.4510]]) tensor([1.5564, 1.2618, 1.4510])\n",
      "tensor(4.5898, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# MSE 사용법 ( torch.nn.function )\n",
    "import torch.nn.functional as F\n",
    "\n",
    "loss = F.mse_loss(z, y)\n",
    "loss.backward()\n",
    "print(w.grad, b.grad)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a53d898c-6a24-447a-911a-ee01c0002a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor(4.5898, grad_fn=<MseLossBackward0>) tensor([2.3346, 1.8927, 2.1765], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "2 tensor(2.0399, grad_fn=<MseLossBackward0>) tensor([1.5564, 1.2618, 1.4510], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "3 tensor(0.9066, grad_fn=<MseLossBackward0>) tensor([1.0376, 0.8412, 0.9673], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "4 tensor(0.4029, grad_fn=<MseLossBackward0>) tensor([0.6917, 0.5608, 0.6449], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "5 tensor(0.1791, grad_fn=<MseLossBackward0>) tensor([0.4611, 0.3739, 0.4299], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n",
      "6 tensor(0.0796, grad_fn=<MseLossBackward0>) tensor([0.3074, 0.2492, 0.2866], grad_fn=<AddBackward0>) tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 업데이트 방법\n",
    "learning_rate = 0.1\n",
    "threshold = 0.1 # loss가 0.1보다 낮아지면 멈추기\n",
    "iteration_num = 0\n",
    "\n",
    "while loss > threshold: # loss가 0.1보다 낮아지면 멈추기\n",
    "    iteration_num += 1\n",
    "    w = w - learning_rate * w.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    print(iteration_num, loss, z, y)\n",
    "    \n",
    "    w.detach_().requires_grad_(True) # 기존 연산에서 끊어줘야 함, 안하면 1회차 때 연산이 2회차때 2회차때는 3회차때 ... 관여함\n",
    "    b.detach_().requires_grad_(True) # detach_().requires_grad_(True)가 연산을 끊는 역할을 함 -> chain rule 중간에 끊는 역할    \n",
    "    z = torch.matmul(x,w) + b\n",
    "    loss = F.mse_loss(z, y)\n",
    "    loss.backward()\n",
    "\n",
    "print(iteration_num + 1, loss, z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7bb33e-9127-452c-a9b1-5333cc061dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
