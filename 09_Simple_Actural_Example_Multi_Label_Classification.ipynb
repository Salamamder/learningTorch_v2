{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d2278ab-09a5-4f12-ac81-11dc34f61fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53bc2017-a748-4cc3-8112-a8afed0a2c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "993a6187-272e-43b1-a633-ca4d512b3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris['data']\n",
    "y = iris['target']\n",
    "names = iris['target_names']\n",
    "feature_names = iris['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b10bc9fc-8164-453c-be57-6c05e8abc9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = q(x, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc69629-6dc1-480b-8ad7-a4e8180d5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(x_train)\n",
    "x_train_tensor = torch.from_numpy(std_scaler.transform(x_train)).float()\n",
    "x_test_tensor = torch.from_numpy(std_scaler.transform(x_test)).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).long()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b7484f-0c3e-4663-a30d-e96b31c554d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([120, 4]) torch.Size([30, 4]) torch.Size([120]) torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tensor.shape, x_test_tensor.shape, y_train_tensor.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c1620c-4a75-4970-b95f-a6a2ba19bf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 1000 # 에폭 1000회\n",
    "minibatch_size = 120 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "302516a0-50ee-4f69-8c17-b729e32079e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, 100),\n",
    "            nn.LeakyReLU(0, 1),\n",
    "            nn.Linear(100,20),\n",
    "            nn.LeakyReLU(0,1),\n",
    "            nn.Linear(20,5),\n",
    "            nn.LeakyReLU(0,1),\n",
    "            nn.Linear(5, output_dim),\n",
    "            # nn.Softmax(dim=-1)\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear_layers(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9f2701d-7f3f-4b8f-98b4-a8dc588d9490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3\n"
     ]
    }
   ],
   "source": [
    "input_dim = x_train_tensor.size(-1)\n",
    "output_dim = 3 # iris는 0, 1, 2 multi label에 대한 확률값을 구해야 하므로, output_dim이 3이 된다.\n",
    "print(input_dim, output_dim)\n",
    "model = Model(input_dim, output_dim)\n",
    "\n",
    "# loss_func = nn.CrossEntropyLoss() # softmax는 CrossEntropyLoss() 로 진행해야 한다.\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7971bc51-8207-4e30-a054-524f8a25e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0310, grad_fn=<DivBackward0>)\n",
      "100 tensor(0.0294, grad_fn=<DivBackward0>)\n",
      "200 tensor(0.0274, grad_fn=<DivBackward0>)\n",
      "300 tensor(0.0251, grad_fn=<DivBackward0>)\n",
      "400 tensor(0.0230, grad_fn=<DivBackward0>)\n",
      "500 tensor(0.0210, grad_fn=<DivBackward0>)\n",
      "600 tensor(0.0193, grad_fn=<DivBackward0>)\n",
      "700 tensor(0.0175, grad_fn=<DivBackward0>)\n",
      "800 tensor(0.0160, grad_fn=<DivBackward0>)\n",
      "900 tensor(0.0149, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for index in range(nb_epochs):\n",
    "    indices = torch.randperm(x_train_tensor.size(0))\n",
    "\n",
    "    x_batch_list = torch.index_select(x_train_tensor, 0, index = indices) # 섞기\n",
    "    y_batch_list = torch.index_select(y_train_tensor, 0, index = indices)\n",
    "    x_batch_list = x_batch_list.split(minibatch_size, 0) # 미니배치 사이즈로 나누기\n",
    "    y_batch_list = y_batch_list.split(minibatch_size, 0)\n",
    "\n",
    "    epoch_loss = list()\n",
    "    for x_minibatch, y_minibatch in zip(x_batch_list, y_batch_list):\n",
    "        y_minibatch_pred = model(x_minibatch)\n",
    "        loss = loss_func(y_minibatch_pred, y_minibatch)\n",
    "        epoch_loss.append(loss)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if(index%100) == 0:\n",
    "        print(index, sum(epoch_loss) / len(epoch_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b0fa0cf-0999-4d1b-8d31-68648a62b499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트셋 기반 evaluation\n",
    "model.eval() # 평가모드로 전환\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model(x_test_tensor) # 여기서 확률값이 나옴\n",
    "    y_pred_list = torch.argmax(y_test_pred, dim=1) # 확률값에서 큰거 골라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "edc192c8-9adf-4483-8283-20bdf49c0cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 3])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# mini_batch size 기반 예측\n",
    "y_pred_list = list()\n",
    "x_test_batch_list = x_test_tensor.split(minibatch_size, 0)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x_minibatch in x_test_batch_list:\n",
    "        y_test_pred = model(x_minibatch)\n",
    "        print(y_test_pred.shape)\n",
    "        y_test_pred = torch.argmax(y_test_pred, dim=1)\n",
    "        print(y_test_pred.shape)\n",
    "        # y_pred_list.extend(y_test_pred.squeeze().detach().tolist()) # 사실 여기서 squeeze를 할 필요는 없음\n",
    "        y_pred_list.extend(y_test_pred.detach().tolist())\n",
    "# y_pred_list = torch.tensor(y_pred_list).unsqueeze(1) # 여기서도 unsqueeze를 할 필요는 없다\n",
    "y_pred_list = torch.tensor(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b0b956c-ce18-46f8-a16e-fa89634d4f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30]) torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "print(y_pred_list.shape, y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f7d86cd-8f27-43e5-89c1-8c4bd44cf2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Precision List:\t [1. 1. 1.]\n",
      "Macro Precision:\t 1.0\n",
      "Macro Precision Formula: 1.0\n",
      "Micro Precision:\t 1.0\n",
      "Recall List:\t [1. 1. 1.]\n",
      "Macro Recall:\t 1.0\n",
      "Micro Recall:\t 1.0\n",
      "Macro F1 Score List:\t [1. 1. 1.]\n",
      "Macro F1 Score:\t 1.0\n",
      "Micro F1 Score:\t 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"Confusion Matrix\\n\", str(confusion_matrix(y_test_tensor, y_pred_list)))\n",
    "print(\"Precision List:\\t\", str( precision_score(y_test_tensor, y_pred_list, average=None) ) )\n",
    "print(\"Macro Precision:\\t\", str( precision_score(y_test_tensor, y_pred_list, average='macro' ) ) )\n",
    "print (\"Macro Precision Formula:\", str( sum(precision_score(y_test_tensor, y_pred_list, average=None) ) / 3 )) # macro 계산을 수동으로 만든\n",
    "print(\"Micro Precision:\\t\", str( precision_score(y_test_tensor, y_pred_list, average='micro') ) )\n",
    "\n",
    "print(\"Recall List:\\t\", str( precision_score(y_test_tensor, y_pred_list, average=None) ) )\n",
    "print(\"Macro Recall:\\t\", str( recall_score(y_test_tensor, y_pred_list, average='macro') ) )\n",
    "print(\"Micro Recall:\\t\", str( recall_score(y_test_tensor, y_pred_list, average='micro') ) )\n",
    "\n",
    "print(\"Macro F1 Score List:\\t\", str( f1_score(y_test_tensor, y_pred_list, average=None) ) )\n",
    "print(\"Macro F1 Score:\\t\", str( f1_score(y_test_tensor, y_pred_list, average='macro') ) )\n",
    "print(\"Micro F1 Score:\\t\", str( f1_score(y_test_tensor, y_pred_list, average='micro') ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a478389-b56b-4620-aa2a-91c1e93f97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "# 과적홥 되는거 같으면 중간에 멈춰주는 역"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
