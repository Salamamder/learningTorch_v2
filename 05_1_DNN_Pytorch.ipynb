{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63395eab-3a86-4ba2-9ade-0754891e6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # 명시적으로 인자명을 써주는 경우도 많이 쓰인다고 한다.\n",
    "        self.linear = nn.Linear(in_features = input_dim, out_features = output_dim)\n",
    "        self.activation = nn.ReLU() # 시그모이드 함수\n",
    "        # self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x)) # 리니어 통과 후 활성화 통"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b218765-9acd-49aa-99af-2f253ea04a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4) # input\n",
    "y = torch.zeros(3) # out\n",
    "model = LinearRegressionModel(4,3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba50662-0500-41ac-a206-4d8c81ddfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f70bf84-8706-436a-8557-71414abbf74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3981e-14, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.3151, -0.2600, -0.2867, -0.3219],\n",
      "        [-0.3371, -0.3982, -0.2403, -0.2803],\n",
      "        [-0.0550,  0.1615, -0.4580, -0.0344]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4923, -0.3556,  0.3860], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss) # sigmoid에 비해 relu가 엄청나게 오차를 줄인다. \n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11d089-8b82-4fd7-997c-0ec47cd9a26b",
   "metadata": {},
   "source": [
    "## 다층 레이어 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "920601b4-60a8-45a6-8b9b-f4f381560b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 10)\n",
    "        self.linear2 = nn.Linear(10, 10)\n",
    "        self.linear3 = nn.Linear(10, 10)\n",
    "        self.linear4 = nn.Linear(10, output_dim)\n",
    "        self.activation = nn.LeakyReLU(0, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #|x| = (input_dim, output_dim)\n",
    "        hidden = self.activation(self.linear1(x))\n",
    "        hidden = self.activation(self.linear2(hidden))\n",
    "        hidden = self.activation(self.linear3(hidden))\n",
    "        y = self.linear4(hidden) # 마지막 출력에는 activation 함수를 사용하지 않는 것이 일반적\n",
    "        return y # 가시성을 위해 분리해서 쓰는겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0d248d-313e-4e08-b8b0-eab9ab073885",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "model = LinearRegressionModel(4, 3)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b8a17c-6d53-4802-8cd3-08bb0d53833f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(nb_epochs+1):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d3e9225-8372-47d5-861d-d834ed4718e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1617e-09, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.3642, -0.1485, -0.4877,  0.4261],\n",
      "        [ 0.2456, -0.2232,  0.3042,  0.2432],\n",
      "        [ 0.2371, -0.2955,  0.0424, -0.3929],\n",
      "        [ 0.3522, -0.4668,  0.0983,  0.0261],\n",
      "        [ 0.3196,  0.1378,  0.1978,  0.0635],\n",
      "        [-0.3038,  0.4129, -0.2057,  0.4990],\n",
      "        [ 0.2223, -0.4499, -0.1757, -0.2742],\n",
      "        [ 0.1448, -0.0907, -0.2221,  0.3788],\n",
      "        [-0.3172, -0.0418,  0.0395,  0.0570],\n",
      "        [-0.3309, -0.2399,  0.1481,  0.0273]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4369, -0.3841,  0.0208, -0.0433, -0.4718, -0.4855,  0.2350,  0.1000,\n",
      "        -0.3320,  0.0508], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1431,  0.2658, -0.0840,  0.0775,  0.3023, -0.2889, -0.0925, -0.2253,\n",
      "         -0.1855,  0.2317],\n",
      "        [-0.2001, -0.1731, -0.1768, -0.1937, -0.2679,  0.0818,  0.0994,  0.2800,\n",
      "         -0.2285, -0.1274],\n",
      "        [-0.1450, -0.2877,  0.1533, -0.0866,  0.2098,  0.2822,  0.0679, -0.2162,\n",
      "          0.1266,  0.2604],\n",
      "        [-0.2089,  0.2431,  0.3054, -0.0569, -0.2586, -0.1365,  0.1385, -0.2602,\n",
      "          0.2981, -0.2792],\n",
      "        [ 0.2366, -0.3083, -0.2872, -0.3051,  0.0930, -0.1311, -0.0593,  0.2986,\n",
      "         -0.2323, -0.0185],\n",
      "        [ 0.0495, -0.0667,  0.2417,  0.1522, -0.2977, -0.0459, -0.0108, -0.2413,\n",
      "          0.1037,  0.2563],\n",
      "        [ 0.1513,  0.2287, -0.2452, -0.0113,  0.2545, -0.0419, -0.0745, -0.2489,\n",
      "         -0.2498,  0.1274],\n",
      "        [-0.2927, -0.0183, -0.2108, -0.2979,  0.1408, -0.0904, -0.2657, -0.0931,\n",
      "         -0.3117, -0.0497],\n",
      "        [-0.0597, -0.2648, -0.1534,  0.1579,  0.0229, -0.2377,  0.0984,  0.0685,\n",
      "          0.1528, -0.1833],\n",
      "        [-0.0136,  0.2819,  0.2755, -0.1237,  0.2241, -0.1719,  0.1272, -0.1233,\n",
      "          0.1834,  0.0257]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.3051, -0.0009,  0.2228,  0.2709, -0.2268, -0.2780,  0.1882,  0.0711,\n",
      "         0.2526, -0.1691], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1786, -0.1609, -0.0408, -0.0724, -0.0163, -0.0976,  0.1780, -0.1776,\n",
      "         -0.2174, -0.0589],\n",
      "        [ 0.3071,  0.1241, -0.2131, -0.1265,  0.0163, -0.2581, -0.1564, -0.2000,\n",
      "          0.1431, -0.1523],\n",
      "        [ 0.0026, -0.2889, -0.0688, -0.0714, -0.2398, -0.2983,  0.1848, -0.3073,\n",
      "         -0.2435,  0.2794],\n",
      "        [-0.0891,  0.1533,  0.2318,  0.0780, -0.0137, -0.1347, -0.3001,  0.0875,\n",
      "         -0.0359,  0.1157],\n",
      "        [ 0.1548, -0.2416,  0.1691, -0.1009, -0.0451,  0.2332, -0.0468,  0.2398,\n",
      "          0.3106,  0.0230],\n",
      "        [ 0.1003,  0.0209,  0.2553, -0.0820, -0.0125,  0.2295,  0.0236, -0.2133,\n",
      "          0.0382,  0.0504],\n",
      "        [ 0.2308, -0.0729, -0.2242, -0.1687, -0.0968,  0.1656, -0.3060,  0.1395,\n",
      "          0.2559,  0.2647],\n",
      "        [ 0.0804, -0.2458,  0.0933, -0.0581, -0.1304, -0.3111, -0.2427, -0.1406,\n",
      "          0.1257,  0.0392],\n",
      "        [ 0.1044,  0.1761, -0.2870, -0.2165, -0.0022,  0.1651, -0.2618,  0.1114,\n",
      "          0.2599,  0.1978],\n",
      "        [-0.2591, -0.0547,  0.2119,  0.0729, -0.1592,  0.2434,  0.2985,  0.0766,\n",
      "         -0.1699,  0.0059]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0079, -0.0965, -0.1893, -0.1008,  0.3071, -0.2140, -0.0896,  0.0294,\n",
      "        -0.0813, -0.1955], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1517, -0.2251,  0.2046,  0.1255,  0.3206,  0.0454,  0.1911,  0.1532,\n",
      "          0.0562,  0.1419],\n",
      "        [-0.0702,  0.3066, -0.0634,  0.1313,  0.1591, -0.1067,  0.1478, -0.3056,\n",
      "         -0.1648, -0.0281],\n",
      "        [ 0.1403, -0.1436, -0.0352,  0.1639, -0.3140, -0.2013,  0.2495,  0.2778,\n",
      "         -0.0309, -0.1479]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1266, -0.0629,  0.1240], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss) # sigmoid에 비해 relu가 엄청나게 오차를 줄인다. \n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee07085b-a671-4631-9a6f-fdeece36f2e5",
   "metadata": {},
   "source": [
    "## nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abdc6a82-6c30-4ac2-8f4b-86442bac1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이거 이용하면 더 쉽게 사용 가능하다\n",
    "x = torch.ones(4)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "input_dim = x.size(0)\n",
    "output_dim = y.size(0)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 10),\n",
    "    nn.LeakyReLU(0, 1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0, 1),\n",
    "    nn.Linear(10, 10),\n",
    "    nn.LeakyReLU(0, 1),\n",
    "    nn.Linear(10, output_dim)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf2d9974-9971-4bd2-9bec-bb150d24f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()\n",
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "302808eb-193b-42d3-9f50-da69cb575701",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8e6cb55-c241-4f9a-9fc5-68e8c1c27685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7967e-11, grad_fn=<MseLossBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1235, -0.0363, -0.2574,  0.3882],\n",
      "        [-0.3704,  0.2853, -0.0282,  0.4459],\n",
      "        [ 0.1817,  0.4610,  0.2297, -0.0395],\n",
      "        [-0.3471, -0.2642,  0.3204, -0.4785],\n",
      "        [-0.4829, -0.4946, -0.3506, -0.2117],\n",
      "        [ 0.0484, -0.1762, -0.2569,  0.0910],\n",
      "        [ 0.4014, -0.2842,  0.0772,  0.2172],\n",
      "        [-0.0371,  0.1607,  0.2857,  0.4682],\n",
      "        [-0.0713,  0.4746,  0.4051,  0.3404],\n",
      "        [ 0.1641,  0.4293, -0.1643, -0.2497]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4962, -0.0631, -0.0854, -0.1095, -0.3312,  0.2602, -0.1722, -0.3223,\n",
      "         0.1068,  0.2292], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0130, -0.0036, -0.1720, -0.3079, -0.0130, -0.1700,  0.1768,  0.2610,\n",
      "          0.0626, -0.1787],\n",
      "        [-0.1124,  0.2790, -0.1211,  0.1322,  0.2129, -0.2442, -0.0308, -0.0252,\n",
      "          0.0732, -0.2221],\n",
      "        [ 0.1126,  0.1695, -0.2334, -0.2124,  0.2976, -0.2676,  0.0406, -0.2633,\n",
      "          0.0727,  0.2776],\n",
      "        [ 0.2885, -0.1875,  0.0228, -0.2843,  0.0689, -0.2384, -0.1951, -0.1677,\n",
      "          0.1220, -0.1933],\n",
      "        [-0.0428, -0.1560, -0.1760, -0.0773,  0.2227,  0.0313, -0.1120, -0.0699,\n",
      "          0.2793,  0.2956],\n",
      "        [-0.0594,  0.1816, -0.2592,  0.0659,  0.0585, -0.0208,  0.0851, -0.1213,\n",
      "         -0.0376, -0.0454],\n",
      "        [ 0.0149,  0.3155,  0.1722, -0.3058,  0.2597, -0.0355, -0.0893, -0.0465,\n",
      "          0.2106,  0.0748],\n",
      "        [-0.1277,  0.0748, -0.2606,  0.0256,  0.0375, -0.3149, -0.2013,  0.2238,\n",
      "         -0.0420, -0.2222],\n",
      "        [ 0.0747, -0.2125,  0.1300, -0.2189, -0.0231, -0.1702, -0.2462, -0.2288,\n",
      "          0.0282, -0.0306],\n",
      "        [ 0.2597, -0.1668,  0.1531, -0.3124,  0.3064, -0.0140,  0.1539,  0.2979,\n",
      "         -0.2199, -0.1284]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0518, -0.2190,  0.2953, -0.1298,  0.2764, -0.1411,  0.1778,  0.1646,\n",
      "        -0.1945,  0.0654], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1101, -0.0360,  0.1703, -0.2501, -0.2281, -0.2404, -0.1940,  0.0914,\n",
      "         -0.0549,  0.1557],\n",
      "        [ 0.2531,  0.2213, -0.0947,  0.0180, -0.2619, -0.2732,  0.3030,  0.2994,\n",
      "          0.0902, -0.2622],\n",
      "        [ 0.0698,  0.1151, -0.1232, -0.0963, -0.1730,  0.2063, -0.1352, -0.0241,\n",
      "          0.0706,  0.1418],\n",
      "        [ 0.3049,  0.1230,  0.0086, -0.0216,  0.1932,  0.2362, -0.1312, -0.3120,\n",
      "          0.0405, -0.1356],\n",
      "        [ 0.0869, -0.0772, -0.0252, -0.2336, -0.1510,  0.3081,  0.0716, -0.2023,\n",
      "         -0.2722, -0.0892],\n",
      "        [-0.1198, -0.1834,  0.2358,  0.1078,  0.2631,  0.0629,  0.0291,  0.2933,\n",
      "         -0.0005,  0.1565],\n",
      "        [-0.0973, -0.0721, -0.0074, -0.1847,  0.1124, -0.0953, -0.1958, -0.1558,\n",
      "         -0.2387,  0.2952],\n",
      "        [ 0.0347,  0.0839,  0.2086, -0.0875,  0.3036, -0.2185,  0.0136, -0.2173,\n",
      "          0.1227,  0.0754],\n",
      "        [-0.2665, -0.2345,  0.0877,  0.3144, -0.0161,  0.1608,  0.2555,  0.0825,\n",
      "          0.0478, -0.0956],\n",
      "        [-0.1363,  0.1453, -0.3019,  0.0796, -0.0439,  0.0436, -0.0220, -0.3159,\n",
      "          0.1540, -0.0793]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1466,  0.2876,  0.1683,  0.2072,  0.2405, -0.2978, -0.0829,  0.2309,\n",
      "        -0.2412,  0.0589], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1028,  0.0252,  0.2597,  0.0120, -0.2244, -0.2981,  0.0105, -0.0036,\n",
      "         -0.0261,  0.3037],\n",
      "        [-0.1087,  0.3030, -0.1665, -0.0214,  0.0045,  0.1792,  0.0194,  0.0642,\n",
      "          0.2689,  0.2141],\n",
      "        [-0.0430, -0.1626, -0.1443,  0.0100,  0.0363,  0.0399, -0.0554, -0.2865,\n",
      "          0.2392,  0.1511]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0374, -0.1311,  0.1753], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c78f50d-566d-46bc-86c5-27e22562f6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
